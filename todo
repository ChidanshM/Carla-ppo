TODO:
[ ] Run old reward functions again for comparison
[x] Try to train a model where we repeat the hard parts instead of resetting to start, fail faster
[x] Measure length of track
[ ] Do some analysis of putting noise on the horizon
  - Might have to fix loading logic
[ ] Train VAE directly on semantic map of road and lane lines (no buildings)
  - Compare performance. Hypothesis: Should drive better as vae does not care about buildings
[ ] Create another environment were we try to follow a pre-determined path
[ ] Change environment to use nodes along the path rather than closest point


# Reward v1:
# - Normalized speed reward [0, 1] until 10 kmh, then 0 + normalized centering reward [0, 1] x 0.5
# - Target speed = 10
# - Std = 0.1
# Results:
# - Learns to drive with an average speed right under the target speed
# - But 10 kmh is too slow, 20 kmh might be a more desireable speed
"""
if not env.terminal_state:
    norm_speed = 3.6 * speed / 10.0
    if norm_speed <= 1.0:
        reward += norm_speed
    reward += (3.0 - env.distance_from_center) / 3.0 * 0.5
"""

# Reward v1.2:
# - Normalized speed reward [0, 1] until 20 kmh, then 0 + normalized centering reward [0, 1] x 0.5
# - Target speed = 20
# Results:
# - Stuck in local minima with std=0.1 (default)
#   - We see the agent driving off the road as soon as speed > 20
#   - Maybe speed reward signal is too weak; it takes too long
#     for the agent to reach speeds that give rewards greater than centering reward
#   - Potential solution: Could make centering reward a function of speed
# - Tried with std=0.4
#   - Got stuck after 1000 episodes; seems like these reward functions are suseptible to the abrrupt change in reward
#   - Std=0.4 might be better in general
"""
if not env.terminal_state:
    norm_speed = 3.6 * speed / 20.0
    if norm_speed <= 1.0:
        reward += norm_speed
    reward += (3.0 - env.distance_from_center) / 3.0 * 0.5
"""

# Reward v2 (REDO)
#
# | speed | throttle | reward |
# +-------+----------+--------+
# | 0     | 0        |  0     |
# | 1     | 0        |  0     |
# | 1     | 1        |  0     |
# | 0     | 1        |  1     |
# | 2     | 1        | -1     |
# | 0     | 0.5      |  0.5   |
# | 0.5   | 0.5      |  0.25  |
#
# throttle - speed * throttle = throttle * (1 - speed)
# =>
# - More reward when we apply throttle with low speeds than throttle with high speeds
#   + centering reward
# - Target Speed = 10
# - Std = 0.4
# Results:
# - Jerky throttle, since optimal policy is to have low speeds but high throttle
"""
if not env.terminal_state:
    norm_speed = 3.6 * speed / 20.0
    reward += env.vehicle.control.throttle * (1 - norm_speed) * 5
    reward += (3.0 - env.distance_from_center) / 3.0 * 0.5
"""

# Reward v3:
# - Normalized speed reward linearly increasing until 1 at target speed, then decreasing linearly starting at 1 with greater speeds
#   + normalized centering reward [0, 1] x 0.5
# - Target speed = 10, because we allow speed over target up to 20 before negative
# - Std = 0.1
# Results:
# - Best reward function so far
# - Reaches speeds that are close to target speed, but sometimes exceeding it by ~5 kmh
# - Seems more stable than v1, perhaps due to it being "more continous"
"""
target_speed = 10.0 # km/h
if not env.terminal_state:
    norm_speed = 3.6 * speed / target_speed
    reward += np.minimum(norm_speed, 2.0 - norm_speed)
    reward += (3.0 - env.distance_from_center) / 3.0 * 0.5
"""

# Reward v3.2 (REDO)
# - Target speed = 20
# - Std = 0.4
# Results:
# - Also sinusoidal (zig-zag) movement. I suspect the sinusoidal movement comes form the increase
#   in std. Perhaps for all these reward functions, we see zig-zag patterns, because it is the optimal
#   movement pattern (maximizes reward.)
# - Solution: Factor in the angle of the car to the road in the reward formulation
"""
target_speed = 20.0 # km/h
if not env.terminal_state:
    norm_speed = 3.6 * speed / target_speed
    reward += np.minimum(norm_speed, 2.0 - norm_speed)
    reward += (3.0 - env.distance_from_center) / 3.0 * 0.5
"""

# Reward v3.3:
# - Target speed = 10
# - Std=0.4
# Results:
# - Zig-zag, but otherwise better than v3.2
# - Now we know that a high std is the cause
"""
target_speed = 10.0 # km/h
if not env.terminal_state:
    norm_speed = 3.6 * speed / target_speed
    reward += np.minimum(norm_speed, 2.0 - norm_speed)
    reward += (3.0 - env.distance_from_center) / 3.0 * 0.5
"""

# Reward v4
# - Normalized speed reward linearly increasing until 1 at target speed, then decreasing linearly starting at 1 with greater speeds
#   - normalized center offset disincentive [0, 1] x 0.5
# - Target speed = 10
# - Std = 0.1
# Results:
# - As good as v3 (which makes sense, since we are normalizing the returns later on anyway) 
"""
target_speed = 10.0 # km/h
if not env.terminal_state:
    norm_speed = 3.6 * speed / target_speed
    reward += np.minimum(norm_speed, 2.0 - norm_speed)
    reward -= env.distance_from_center / 3.0 * 0.5
"""

# Reward v5
# - Same as v4 but being off-center is disincentivized more than speed is incentivized
# - Target speed = 10
# - Std = 0.1
# Results:
# - Did not learn a good policy, as the speed incentive is not the primary incentive anymore
"""
target_speed = 10.0 # km/h
if not env.terminal_state:
    norm_speed = 3.6 * speed / target_speed
    reward += np.minimum(norm_speed, 2.0 - norm_speed)
    reward -= env.distance_from_center / 3.0 * 2.0
"""

# Reward v6 (REDO)
# - Speed only reward (Kendall)
# - Std = 0.4
# Results:
# - Produced erratic, almost sinusoidal, steering behaviour
#   (perhaps due to a lack of incentive to stay centered)
# - Speed is unbounded, so the agent often reaches speeds of 40 kmh
"""
if not env.terminal_state:
    reward += speed
"""

# Reward v7 (REDO)
# - Centering is more incentivized when speed is higher
# - Theory: Learn to speed up first, then center
# - Note: This was run with CarlaLapEnv and without "terminate on opposite heading"
# - Std = 0.4
# Results:
# - Sinusoidal steering behaviour
# - Speed close to 20 kmh
# - Note: Maybe a consequence of target speed 20 kmh and std 0.4?
"""
target_speed = 20.0 # km/h
if not env.terminal_state:
    norm_speed = 3.6 * speed / target_speed
    norm_speed2 = np.minimum(norm_speed, 2.0 - norm_speed)
    reward += norm_speed2
    reward += (3.0 - env.distance_from_center) / 3.0 * norm_speed2
"""